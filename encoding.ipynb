{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given Imports\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoders and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein encoder\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "\n",
    "# Molecule encoder\n",
    "mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "mol_model = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\").to(device)\n",
    "\n",
    "max_prot_input_size = prot_model.config.max_position_embeddings\n",
    "max_mol_input_size = mol_model.config.max_position_embeddings-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jglaser/binding_affinity\")\n",
    "dataset = dataset[\"train\"].train_test_split(train_size=0.001)['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc58a119bfe1472f8872a5ab2e8c9c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(example):\n",
    "    example['seq'] = \" \".join(re.sub(r\"[UZOB]\", \"X\", example['seq']))\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Tokenizing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoding functions\n",
    "def encode_batch(batch, tokenizer, model, max_input_size):\n",
    "    tokens = tokenizer(batch, padding=True, truncation=True, max_length=max_input_size, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens.to(device)).pooler_output\n",
    "    # representations = outputs.last_hidden_state.mean(dim=1)\n",
    "    return outputs.cpu()\n",
    "\n",
    "def encode_sequences(prot_seq, mol_smiles, mol_batch_size=16, prot_batch_size=2):\n",
    "    # Encode in batches to prevent out-of-memory errors\n",
    "    prot_representations = []\n",
    "    mol_representations = []\n",
    "    \n",
    "    mol_loader = DataLoader(mol_smiles, batch_size=mol_batch_size, shuffle=False)\n",
    "    for i, mol_batch in enumerate(mol_loader, 1):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"\\rEncoding molecule batch {i}/{len(mol_loader)}...\", end=\"\")\n",
    "        mol_representations.append(encode_batch(mol_batch, mol_tokenizer, mol_model, max_mol_input_size))\n",
    "    print(\"done!\")\n",
    "    \n",
    "    mol_model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    prot_loader = DataLoader(prot_seq, batch_size=prot_batch_size, shuffle=False)\n",
    "    for i, prot_batch in enumerate(prot_loader, 1):\n",
    "        print(f\"\\rEncoding protein batch {i}/{len(prot_loader)}...\", end=\"\")\n",
    "        prot_representations.append(encode_batch(prot_batch, prot_tokenizer, prot_model, max_prot_input_size))\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"done!\")\n",
    "    return torch.cat(prot_representations, dim=0), torch.cat(mol_representations, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(dataset):\n",
    "    proteins, smiles, affinities = dataset[\"seq\"], dataset[\"smiles_can\"], dataset[\"affinity\"]\n",
    "    prot_rep, chem_rep = encode_sequences(proteins, smiles)\n",
    "    return TensorDataset(prot_rep, chem_rep, torch.tensor(affinities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding data...\n",
      "Encoding molecule batch 100/115...done!\n",
      "Encoding protein batch 84/918..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tatwo\\Desktop\\Programs\\Wolfram\\WELP-PLAPT\\encoding.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mencoding data...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tensor_dataset \u001b[39m=\u001b[39m create_tensor_dataset(dataset)\n",
      "\u001b[1;32mc:\\Users\\tatwo\\Desktop\\Programs\\Wolfram\\WELP-PLAPT\\encoding.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_tensor_dataset\u001b[39m(dataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     proteins, smiles, affinities \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mseq\u001b[39m\u001b[39m\"\u001b[39m], dataset[\u001b[39m\"\u001b[39m\u001b[39msmiles_can\u001b[39m\u001b[39m\"\u001b[39m], dataset[\u001b[39m\"\u001b[39m\u001b[39maffinity\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     prot_rep, chem_rep \u001b[39m=\u001b[39m encode_sequences(proteins, smiles)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m TensorDataset(prot_rep, chem_rep, torch\u001b[39m.\u001b[39mtensor(affinities))\n",
      "\u001b[1;32mc:\\Users\\tatwo\\Desktop\\Programs\\Wolfram\\WELP-PLAPT\\encoding.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, prot_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(prot_loader, \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mEncoding protein batch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(prot_loader)\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     prot_representations\u001b[39m.\u001b[39mappend(encode_batch(prot_batch, prot_tokenizer, prot_model, max_prot_input_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\tatwo\\Desktop\\Programs\\Wolfram\\WELP-PLAPT\\encoding.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokens\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mpooler_output\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# representations = outputs.last_hidden_state.mean(dim=1)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tatwo/Desktop/Programs/Wolfram/WELP-PLAPT/encoding.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\u001b[39m.\u001b[39;49mcpu()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"encoding data...\")\n",
    "tensor_dataset = create_tensor_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tensor_dataset, r\"C:\\Users\\tatwo\\Downloads\\encoded_data_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4373, -0.6694, -1.0074,  ..., -0.9850,  0.2792,  1.4708],\n",
       "         [-0.1302, -1.5081, -0.6598,  ...,  0.2113,  1.4561,  0.2045],\n",
       "         [ 0.8827, -0.3146, -1.0314,  ...,  1.0429,  1.5200,  0.1877],\n",
       "         ...,\n",
       "         [-0.3223, -1.1277,  0.2969,  ..., -1.9156,  0.3809,  1.1264],\n",
       "         [ 0.0172,  1.5409, -0.9348,  ..., -0.8201,  0.8186, -0.6957],\n",
       "         [-1.4186, -1.0689,  0.3970,  ..., -0.7573,  0.7821,  0.7213]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 2.9266e-01, -2.0106e-01, -2.5144e-01,  2.7638e-01, -5.9401e-01,\n",
       "          4.7673e-01,  4.0142e-01,  1.5805e-01, -2.2580e-01, -4.7935e-01,\n",
       "         -6.7652e-01,  1.6724e-02,  9.4531e-01,  3.6973e-01,  3.7976e-01,\n",
       "         -2.0460e-01, -1.0615e-01, -3.7676e-01, -1.8930e-01, -2.3977e-01,\n",
       "          8.3878e-03,  2.7116e-01, -2.4827e-01,  2.4240e-01, -6.5158e-01,\n",
       "         -1.0794e-01, -2.9413e-01, -1.7536e-01,  4.4841e-02, -9.3393e-05,\n",
       "         -1.0005e-01,  5.1792e-02,  4.2853e-01, -1.9518e-01, -3.7019e-01,\n",
       "         -6.4103e-01,  6.4570e-01, -3.3646e-01,  3.8487e-01, -1.3384e-01,\n",
       "          4.4586e-02,  6.1111e-01, -2.3334e-01, -5.1105e-01, -3.2470e-01,\n",
       "         -3.8882e-02,  2.2602e-01, -2.6721e-01, -7.1416e-01, -9.9216e-02,\n",
       "         -6.6145e-01, -6.7018e-02,  6.7712e-01,  4.4566e-01, -6.1443e-01,\n",
       "         -7.4043e-01, -3.6978e-01,  3.1468e-01,  5.8633e-01, -6.9997e-01,\n",
       "         -8.4047e-01,  7.7237e-01,  6.0692e-01,  8.2018e-01,  6.0010e-01,\n",
       "         -4.7667e-04, -6.0803e-01, -1.1620e-02,  8.3403e-01,  2.7374e-01,\n",
       "         -3.8763e-01,  2.8850e-01, -6.3113e-01, -5.7872e-02, -4.4303e-01,\n",
       "         -6.8976e-01, -6.2199e-02, -2.4624e-01, -3.5232e-01,  1.4532e-01,\n",
       "          4.0185e-01,  2.2907e-02,  4.8014e-01,  3.5945e-01,  4.4379e-01,\n",
       "         -3.4230e-03,  5.1531e-01,  2.9359e-01, -4.3855e-01, -1.2571e-01,\n",
       "          2.5080e-01, -1.3384e-01,  4.3975e-03, -4.2842e-01,  5.2732e-01,\n",
       "         -4.5868e-01,  6.5070e-01,  4.3873e-01,  9.0760e-01, -5.2048e-01,\n",
       "         -1.6096e-01,  1.6240e-01,  6.2803e-01,  7.4437e-01, -6.1717e-01,\n",
       "         -1.8278e-01, -4.9924e-01, -5.7683e-01, -2.0259e-01,  1.1569e-01,\n",
       "          4.6273e-02,  5.6951e-01, -5.6528e-01,  2.7900e-01, -4.6983e-01,\n",
       "          1.8460e-01, -6.8210e-01,  6.0106e-01, -3.8628e-01,  2.0334e-01,\n",
       "          2.0808e-01, -4.9167e-01, -9.9243e-02,  5.7946e-02,  1.5097e-01,\n",
       "         -5.4933e-03,  6.6690e-01,  6.3872e-02,  2.1548e-01, -2.5069e-01,\n",
       "          2.4343e-01, -6.2675e-01, -7.5987e-01,  2.6855e-01, -4.4456e-02,\n",
       "         -2.8168e-01,  9.1364e-01, -4.0820e-02, -3.3919e-01,  6.0286e-01,\n",
       "         -2.7431e-01,  5.9292e-01,  2.4104e-01, -4.5743e-01, -1.6944e-02,\n",
       "          1.7253e-01,  3.4105e-01, -5.3371e-02, -3.5978e-02, -4.1792e-01,\n",
       "          2.9665e-02,  4.5603e-01,  7.9170e-01,  9.0871e-02,  4.6061e-01,\n",
       "          4.5809e-01, -2.6914e-01,  2.5590e-01,  2.4508e-01,  5.8299e-01,\n",
       "         -3.6979e-01,  6.1066e-01,  2.1603e-01, -5.9620e-01, -6.4709e-01,\n",
       "         -2.9362e-01, -9.4762e-02, -3.0880e-01, -6.8185e-01,  1.9692e-01,\n",
       "          5.2041e-01, -2.5413e-01,  1.0204e-01,  3.6088e-01, -3.2305e-01,\n",
       "          1.1571e-01,  1.7292e-01,  6.0481e-01,  2.9135e-01, -4.5428e-01,\n",
       "         -4.8679e-01,  3.5914e-01,  1.1823e-01, -6.5898e-01, -2.4915e-01,\n",
       "         -5.2720e-01,  6.4213e-01,  7.5813e-01, -3.3928e-01,  2.2837e-01,\n",
       "         -3.4633e-01,  2.9574e-01,  4.6955e-01, -2.0308e-01,  4.6772e-02,\n",
       "          8.1871e-01, -4.8880e-01,  4.1499e-01, -4.5076e-01, -1.4154e-01,\n",
       "          2.5151e-01, -2.5274e-01,  7.8204e-01, -6.3615e-01,  3.7211e-01,\n",
       "          3.5499e-01, -8.8430e-01,  1.2391e-01, -9.4578e-02, -2.4236e-01,\n",
       "         -6.7426e-01, -6.1039e-01, -1.7673e-01, -6.6865e-01, -2.7885e-01,\n",
       "          5.0805e-01, -3.3921e-01,  9.3490e-02,  3.7691e-01, -6.1615e-02,\n",
       "          1.1616e-01, -6.8762e-01, -2.5775e-01,  1.8795e-01, -6.9623e-02,\n",
       "         -2.3428e-01,  2.3471e-01,  2.7699e-01,  4.0835e-01, -6.0206e-01,\n",
       "         -3.1559e-01, -4.4148e-01,  9.2625e-02, -7.7604e-01,  7.5247e-01,\n",
       "         -3.4784e-01, -2.9181e-01, -1.3648e-01, -5.6352e-01,  7.6529e-03,\n",
       "          6.6498e-01, -1.8469e-01,  8.6315e-02,  5.9657e-01, -6.5804e-01,\n",
       "          2.6182e-01, -3.7584e-01, -4.7825e-01,  3.9715e-01, -1.7592e-01,\n",
       "          2.6041e-01,  1.7894e-01, -1.5762e-02,  3.3932e-02, -6.4229e-02,\n",
       "         -1.2057e-01,  3.4020e-01,  3.0019e-01, -3.5586e-01, -3.9305e-01,\n",
       "         -8.8588e-01, -8.0750e-02, -6.8382e-01,  5.7716e-01, -8.8196e-01,\n",
       "         -4.2831e-01, -1.9758e-01, -5.2855e-01,  2.9176e-01,  7.3715e-01,\n",
       "          1.4874e-01, -3.6739e-02, -1.3302e-01, -6.9399e-01,  2.9320e-01,\n",
       "         -4.5998e-01,  5.5820e-01, -1.8237e-01,  4.7170e-02,  2.7974e-01,\n",
       "         -2.6873e-01, -1.6112e-01,  7.9306e-01, -7.5923e-01, -6.6097e-01,\n",
       "         -6.0324e-02,  2.9259e-01,  8.3814e-01,  6.9768e-02,  2.9655e-01,\n",
       "          6.8221e-01, -7.2223e-01, -1.4918e-01, -9.0342e-01, -4.9295e-01,\n",
       "          3.9917e-01,  4.0683e-01, -2.9660e-01,  2.6952e-01, -3.6836e-01,\n",
       "         -3.4453e-01, -2.5244e-01,  9.2252e-01,  4.2580e-01, -2.3097e-01,\n",
       "         -2.3022e-01,  3.4272e-02, -5.5838e-01, -1.7409e-01,  2.0257e-02,\n",
       "         -9.5129e-02, -2.4292e-01,  6.1842e-01, -6.5253e-01,  5.2981e-01,\n",
       "          6.4060e-01, -6.9315e-01,  3.2877e-01, -7.6624e-01,  1.7028e-01,\n",
       "         -1.3934e-01,  3.4675e-01,  2.9761e-01,  5.0055e-01,  1.4617e-01,\n",
       "          6.6818e-02, -3.1182e-01, -3.5610e-01,  4.5260e-01,  1.0907e-01,\n",
       "          8.8470e-01,  7.1847e-01,  3.7007e-01, -3.9098e-01,  5.6396e-01,\n",
       "         -2.1773e-01, -6.0347e-01,  1.7909e-01,  1.1723e-01,  2.4811e-01,\n",
       "          4.6843e-01,  1.8712e-01,  1.6246e-01, -7.5788e-02,  2.6950e-01,\n",
       "          8.6987e-03, -5.3490e-01,  4.0632e-01, -7.1292e-01, -3.7946e-01,\n",
       "          5.2616e-01,  1.1131e-01, -6.7565e-01,  4.0259e-01, -1.2211e-01,\n",
       "         -1.1306e-02,  7.5006e-02, -3.3856e-01,  4.0554e-01, -9.3221e-02,\n",
       "         -6.3623e-01,  4.9200e-01,  1.1055e-01, -2.6392e-01,  1.6519e-01,\n",
       "          2.7618e-01, -3.0051e-01,  5.8814e-01,  3.5151e-03, -7.8505e-01,\n",
       "         -2.0492e-01,  6.0156e-01,  5.5387e-01,  3.2052e-02,  1.2759e-01,\n",
       "         -7.0448e-01,  4.0338e-01, -1.2790e-01,  5.2428e-01,  1.7253e-01,\n",
       "         -6.7519e-01, -5.3529e-01,  4.4248e-01, -7.0234e-02,  6.8174e-01,\n",
       "          4.6045e-02,  6.7626e-01, -5.1410e-01, -2.6933e-01, -1.4872e-01,\n",
       "          8.3963e-01,  3.0456e-01, -2.6591e-01,  3.4989e-01,  3.1684e-02,\n",
       "          5.2450e-03, -6.6699e-01, -3.2902e-01,  5.2520e-01, -1.1680e-01,\n",
       "          7.6002e-01, -4.0114e-01, -8.8250e-02, -3.8436e-01, -7.9385e-01,\n",
       "         -2.3931e-01,  1.7622e-01,  3.1946e-01,  5.7268e-01,  1.4804e-01,\n",
       "          2.0495e-01,  7.7191e-01, -4.7492e-01, -4.6505e-02, -3.7238e-01,\n",
       "         -3.0060e-01, -3.9502e-02, -3.8723e-01,  1.1616e-01, -3.9340e-01,\n",
       "         -1.6677e-01, -4.8683e-01, -4.3036e-01,  2.3243e-01, -1.6634e-01,\n",
       "         -8.5488e-02,  5.6730e-02, -4.1403e-01, -2.2940e-01, -4.4725e-03,\n",
       "          3.1065e-01, -2.5698e-01,  6.2466e-01,  5.3190e-01, -4.0741e-01,\n",
       "          5.5202e-01,  7.6314e-03, -1.4648e-01, -4.9081e-01,  8.3137e-01,\n",
       "         -4.9703e-01, -4.6178e-01, -2.8109e-01,  4.9070e-01,  6.4874e-01,\n",
       "         -8.1448e-01,  3.8059e-01, -2.4878e-01, -2.1170e-01, -1.7103e-01,\n",
       "          1.7325e-01, -9.6499e-02,  4.3680e-01, -3.1189e-01, -1.6027e-01,\n",
       "          6.8473e-01, -7.1751e-01, -5.7290e-01, -1.0337e-01, -7.7214e-01,\n",
       "          2.7153e-01, -4.7598e-01, -5.9594e-01,  2.7803e-01,  1.2125e-01,\n",
       "         -2.6909e-01, -3.5186e-01,  2.3822e-01,  3.6605e-01,  8.6451e-02,\n",
       "         -2.3979e-02, -3.3021e-01,  8.2654e-01, -5.0671e-01,  8.4709e-01,\n",
       "          3.4495e-01,  5.6795e-01, -4.8558e-01, -2.1427e-01,  5.4782e-01,\n",
       "          1.2531e-01, -5.8110e-01, -3.7883e-01,  7.7166e-01, -4.2583e-01,\n",
       "         -2.1068e-01,  3.3740e-01, -5.0829e-02, -5.8291e-01,  7.5717e-01,\n",
       "          7.6367e-01,  4.2239e-02,  5.0345e-01, -4.6831e-01,  1.4459e-01,\n",
       "         -6.1693e-01, -4.5239e-01,  5.7888e-01, -3.0008e-01, -2.1321e-01,\n",
       "         -2.8683e-01, -4.1005e-01,  1.5732e-01,  5.8152e-01, -8.6716e-01,\n",
       "          3.7029e-01,  5.9108e-02,  5.2019e-01,  4.0457e-01, -6.4090e-01,\n",
       "          2.2311e-01, -1.7763e-01,  8.8409e-02,  3.9174e-01,  2.9132e-02,\n",
       "         -3.8701e-01,  1.8685e-01, -5.8972e-01,  2.9444e-01, -5.0521e-01,\n",
       "          4.2299e-02, -2.3830e-01, -7.6446e-02,  2.1243e-01, -3.9048e-01,\n",
       "          1.5059e-02, -7.3542e-02,  6.2636e-01, -8.3288e-02, -2.8523e-01,\n",
       "          7.8642e-01, -3.7587e-01,  5.8801e-01,  5.5305e-01,  6.8738e-01,\n",
       "          8.4744e-02,  1.9354e-02,  5.9951e-01,  6.9784e-01,  2.4600e-01,\n",
       "         -7.5630e-01,  2.8804e-01,  6.2338e-01, -4.5703e-01, -4.7392e-01,\n",
       "          3.7606e-01,  2.1875e-01,  6.5555e-01,  8.1756e-01,  7.1803e-01,\n",
       "         -6.8440e-01, -5.3071e-02, -5.1707e-01,  3.6397e-01, -4.4673e-01,\n",
       "          1.9359e-01,  4.3042e-01,  6.4816e-01, -3.9844e-01,  3.8398e-01,\n",
       "          7.7087e-02,  2.3044e-01, -9.1848e-01, -6.2665e-01,  2.1387e-01,\n",
       "          7.8288e-02, -7.4854e-02,  7.4908e-01, -5.5568e-01, -4.7569e-01,\n",
       "         -5.5587e-01, -1.4950e-01,  5.7176e-02,  1.6273e-01, -6.3316e-01,\n",
       "         -7.1496e-02, -1.6120e-01,  1.3784e-01,  4.2198e-01, -2.0397e-01,\n",
       "         -2.9214e-01, -1.9454e-01,  1.6272e-01, -2.1364e-01,  5.5222e-01,\n",
       "         -8.7578e-01,  4.1631e-01,  8.4734e-01, -2.9092e-01,  2.8257e-01,\n",
       "         -7.7537e-01,  7.3905e-01,  6.0575e-01, -4.5452e-01, -8.1860e-01,\n",
       "         -4.0959e-01,  4.6899e-01, -5.1574e-01,  7.0919e-01,  1.1515e-01,\n",
       "          3.9174e-01,  3.1415e-01,  5.4840e-01, -1.0543e-01,  1.3218e-01,\n",
       "          7.5969e-01,  3.3672e-01, -3.6808e-01,  1.5985e-01,  1.5620e-01,\n",
       "          1.9492e-01, -7.1636e-01,  3.7514e-01,  8.7104e-02, -6.6841e-01,\n",
       "          2.6789e-01, -7.1122e-01, -4.3491e-01, -2.8127e-01, -4.8617e-01,\n",
       "         -7.4943e-02, -1.1696e-01, -2.8435e-01, -8.4809e-01, -5.1714e-01,\n",
       "         -9.1303e-01,  2.4577e-01,  6.4478e-01,  5.9679e-01,  2.2563e-01,\n",
       "         -2.5681e-01,  1.9443e-01,  3.9242e-01, -9.9936e-02, -5.4072e-01,\n",
       "          1.7852e-01, -9.0268e-02,  6.0166e-01,  4.9137e-01, -8.9963e-01,\n",
       "          7.8463e-01,  4.9963e-01,  1.5921e-01, -5.3403e-01, -1.8653e-01,\n",
       "          7.4914e-01, -7.7274e-01, -5.5907e-01,  5.4732e-01,  7.0108e-02,\n",
       "          4.0162e-01,  6.8716e-01,  2.6635e-01, -3.4943e-01,  2.4823e-01,\n",
       "         -5.2866e-01, -6.7906e-01,  2.3657e-01,  2.7691e-01,  2.3464e-01,\n",
       "          7.3293e-02,  7.4249e-01,  4.7609e-01, -1.1437e-01,  5.2901e-01,\n",
       "          1.1347e-01, -6.7678e-01, -4.2227e-01, -3.5853e-01, -7.8315e-01,\n",
       "         -4.3807e-02, -8.7219e-01, -5.3206e-01, -2.2873e-01, -1.0465e-01,\n",
       "          4.9312e-01,  6.0256e-01,  8.8773e-02, -7.3862e-01, -5.3669e-01,\n",
       "         -4.0186e-01, -6.4815e-01,  6.2954e-04, -2.4779e-01,  6.2436e-01,\n",
       "         -3.1829e-01, -8.5521e-01, -3.8313e-01,  4.0913e-02,  4.8201e-01,\n",
       "         -2.1527e-01, -9.0262e-01, -2.7854e-02,  1.0322e-01,  1.3151e-01,\n",
       "         -5.3416e-01,  5.0795e-01, -7.5807e-02, -1.0240e-01,  2.9667e-01,\n",
       "         -5.5380e-01, -5.7184e-01, -3.2264e-02,  3.4249e-01,  6.6083e-01,\n",
       "         -2.5674e-01, -2.4114e-01, -7.5168e-01, -3.1382e-01, -2.5875e-01,\n",
       "         -4.2257e-01, -9.0772e-02, -7.2822e-02,  1.1924e-01, -4.9435e-01,\n",
       "         -3.6359e-01, -4.8533e-01,  2.8616e-01,  7.3003e-01,  1.5917e-01,\n",
       "         -5.4976e-01,  4.4801e-01,  5.7851e-01,  2.8454e-01, -3.3731e-01,\n",
       "         -6.5820e-01, -1.5477e-01,  1.1745e-01,  7.2023e-02,  2.2826e-01,\n",
       "          3.9326e-01,  9.5315e-02,  5.9001e-01,  6.0741e-01,  1.7065e-01,\n",
       "          2.0512e-01,  2.5990e-01,  1.3177e-01,  3.2254e-01, -9.8649e-02,\n",
       "         -6.2918e-01,  5.0668e-01, -8.1668e-01, -1.1122e-01, -3.6646e-01,\n",
       "         -6.0003e-01,  8.3290e-01,  4.9222e-01, -4.4597e-01, -6.7376e-01,\n",
       "         -8.2182e-01,  4.1201e-01,  1.8402e-01,  3.9823e-01,  8.1154e-01,\n",
       "         -2.2929e-01, -7.2824e-01, -5.5268e-01, -1.8649e-02, -4.8606e-01,\n",
       "         -8.7519e-02,  3.7578e-01, -1.5730e-01, -4.8730e-01,  2.0852e-01,\n",
       "         -6.6081e-01,  6.8942e-01,  5.6557e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=mol_tokenizer([\"CCCCCCCCCCCCCCCCCCCC(=O)O\"], padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "mol_model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
