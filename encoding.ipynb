{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given Imports\n",
    "import torch\n",
    "import re\n",
    "import statistics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoders and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein encoder\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "\n",
    "# Molecule encoder\n",
    "mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "mol_model = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\").to(device)\n",
    "\n",
    "# max_prot_input_size = prot_model.config.max_position_embeddings\n",
    "max_prot_input_size = 3200 #capped at 3000 since tokens longer than 3000 use way too much vram\n",
    "max_mol_input_size = 278 #capped at 278 since the longest tokenized smiles sequence in the dataset has a length of 278. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jglaser/binding_affinity\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset100k = dataset.select(range(100000))\n",
    "dataset10k = dataset.select(range(10000))\n",
    "dataset1k = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess & Tokenize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace irregular amino acids in the dataset's protein sequences with \"X\", which is necessary for accurate tokenization and encodings from the ProtBERT model\n",
    "Parallelized map function is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    import re\n",
    "    example['seq'] = \" \".join(re.sub(r\"[UZOB]\", \"X\", example['seq']))\n",
    "    return example\n",
    "\n",
    "dataset100k = dataset100k.map(preprocess_function, num_proc=8)\n",
    "dataset10k = dataset10k.map(preprocess_function, num_proc=8)\n",
    "dataset1k = dataset1k.map(preprocess_function, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prot(example):\n",
    "    from transformers import BertTokenizer\n",
    "    prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    return prot_tokenizer(example['seq'], padding=True, truncation=True, max_length=max_prot_input_size, return_tensors='pt')\n",
    "\n",
    "def tokenize_mol(example):\n",
    "    from transformers import RobertaTokenizer\n",
    "    mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "    return mol_tokenizer(example['smiles_can'], padding=True, return_tensors='pt')\n",
    "    # return mol_tokenizer(example['smiles_can'], padding=True, truncation=True, max_length=max_mol_input_size, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Tokenizing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoding functions\n",
    "def encode_batch(batch, tokenizer, model, max_input_size):\n",
    "    tokens = tokenizer(batch, padding=True, truncation=True, max_length=max_input_size, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens.to(device)).pooler_output\n",
    "    # representations = outputs.last_hidden_state.mean(dim=1)\n",
    "    return outputs.cpu()\n",
    "\n",
    "def encode_sequences(prot_seq, mol_smiles, mol_batch_size=16, prot_batch_size=2):\n",
    "    # Encode in batches to prevent out-of-memory errors\n",
    "    prot_representations = []\n",
    "    mol_representations = []\n",
    "    \n",
    "    mol_loader = DataLoader(mol_smiles, batch_size=mol_batch_size, shuffle=False)\n",
    "    for i, mol_batch in enumerate(mol_loader, 1):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"\\rEncoding molecule batch {i}/{len(mol_loader)}...\", end=\"\")\n",
    "        mol_representations.append(encode_batch(mol_batch, mol_tokenizer, mol_model, max_mol_input_size))\n",
    "    print(\"done!\")\n",
    "    \n",
    "    mol_model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    prot_loader = DataLoader(prot_seq, batch_size=prot_batch_size, shuffle=False)\n",
    "    for i, prot_batch in enumerate(prot_loader, 1):\n",
    "        print(f\"\\rEncoding protein batch {i}/{len(prot_loader)}...\", end=\"\")\n",
    "        prot_representations.append(encode_batch(prot_batch, prot_tokenizer, prot_model, max_prot_input_size))\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"done!\")\n",
    "    return torch.cat(prot_representations, dim=0), torch.cat(mol_representations, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(dataset):\n",
    "    proteins, smiles, affinities = dataset[\"seq\"], dataset[\"smiles_can\"], dataset[\"affinity\"]\n",
    "    prot_rep, chem_rep = encode_sequences(proteins, smiles)\n",
    "    return TensorDataset(prot_rep, chem_rep, torch.tensor(affinities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding data...\n",
      "Encoding molecule batch 6240/6250...done!\n",
      "Encoding protein batch 50000/50000...done!\n"
     ]
    }
   ],
   "source": [
    "print(\"encoding data...\")\n",
    "tensor_dataset = create_tensor_dataset(dataset100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2664,  0.2810, -0.2537,  ...,  0.2681,  0.2509, -0.2737]),\n",
       " tensor([ 2.9266e-01, -2.0106e-01, -2.5144e-01,  2.7638e-01, -5.9401e-01,\n",
       "          4.7673e-01,  4.0143e-01,  1.5805e-01, -2.2580e-01, -4.7935e-01,\n",
       "         -6.7652e-01,  1.6724e-02,  9.4531e-01,  3.6973e-01,  3.7976e-01,\n",
       "         -2.0460e-01, -1.0615e-01, -3.7676e-01, -1.8930e-01, -2.3976e-01,\n",
       "          8.3889e-03,  2.7116e-01, -2.4827e-01,  2.4240e-01, -6.5158e-01,\n",
       "         -1.0794e-01, -2.9413e-01, -1.7536e-01,  4.4842e-02, -9.2611e-05,\n",
       "         -1.0005e-01,  5.1793e-02,  4.2853e-01, -1.9518e-01, -3.7019e-01,\n",
       "         -6.4103e-01,  6.4570e-01, -3.3646e-01,  3.8487e-01, -1.3384e-01,\n",
       "          4.4586e-02,  6.1111e-01, -2.3334e-01, -5.1104e-01, -3.2470e-01,\n",
       "         -3.8882e-02,  2.2602e-01, -2.6721e-01, -7.1416e-01, -9.9216e-02,\n",
       "         -6.6145e-01, -6.7018e-02,  6.7712e-01,  4.4566e-01, -6.1443e-01,\n",
       "         -7.4043e-01, -3.6978e-01,  3.1468e-01,  5.8633e-01, -6.9997e-01,\n",
       "         -8.4047e-01,  7.7237e-01,  6.0692e-01,  8.2018e-01,  6.0010e-01,\n",
       "         -4.7681e-04, -6.0803e-01, -1.1621e-02,  8.3403e-01,  2.7374e-01,\n",
       "         -3.8764e-01,  2.8850e-01, -6.3113e-01, -5.7872e-02, -4.4303e-01,\n",
       "         -6.8976e-01, -6.2199e-02, -2.4624e-01, -3.5232e-01,  1.4532e-01,\n",
       "          4.0185e-01,  2.2908e-02,  4.8014e-01,  3.5945e-01,  4.4379e-01,\n",
       "         -3.4230e-03,  5.1531e-01,  2.9359e-01, -4.3855e-01, -1.2571e-01,\n",
       "          2.5080e-01, -1.3384e-01,  4.3987e-03, -4.2842e-01,  5.2732e-01,\n",
       "         -4.5868e-01,  6.5070e-01,  4.3873e-01,  9.0760e-01, -5.2048e-01,\n",
       "         -1.6096e-01,  1.6240e-01,  6.2803e-01,  7.4437e-01, -6.1717e-01,\n",
       "         -1.8278e-01, -4.9924e-01, -5.7683e-01, -2.0259e-01,  1.1569e-01,\n",
       "          4.6274e-02,  5.6951e-01, -5.6528e-01,  2.7900e-01, -4.6983e-01,\n",
       "          1.8460e-01, -6.8210e-01,  6.0106e-01, -3.8628e-01,  2.0334e-01,\n",
       "          2.0808e-01, -4.9167e-01, -9.9243e-02,  5.7945e-02,  1.5097e-01,\n",
       "         -5.4930e-03,  6.6690e-01,  6.3872e-02,  2.1548e-01, -2.5069e-01,\n",
       "          2.4343e-01, -6.2675e-01, -7.5987e-01,  2.6855e-01, -4.4456e-02,\n",
       "         -2.8168e-01,  9.1364e-01, -4.0821e-02, -3.3919e-01,  6.0286e-01,\n",
       "         -2.7431e-01,  5.9292e-01,  2.4104e-01, -4.5743e-01, -1.6945e-02,\n",
       "          1.7253e-01,  3.4105e-01, -5.3371e-02, -3.5979e-02, -4.1792e-01,\n",
       "          2.9665e-02,  4.5602e-01,  7.9170e-01,  9.0870e-02,  4.6061e-01,\n",
       "          4.5809e-01, -2.6914e-01,  2.5590e-01,  2.4508e-01,  5.8299e-01,\n",
       "         -3.6979e-01,  6.1066e-01,  2.1603e-01, -5.9620e-01, -6.4709e-01,\n",
       "         -2.9362e-01, -9.4762e-02, -3.0880e-01, -6.8185e-01,  1.9692e-01,\n",
       "          5.2041e-01, -2.5413e-01,  1.0204e-01,  3.6088e-01, -3.2305e-01,\n",
       "          1.1571e-01,  1.7292e-01,  6.0481e-01,  2.9135e-01, -4.5428e-01,\n",
       "         -4.8679e-01,  3.5914e-01,  1.1823e-01, -6.5898e-01, -2.4915e-01,\n",
       "         -5.2720e-01,  6.4213e-01,  7.5813e-01, -3.3928e-01,  2.2837e-01,\n",
       "         -3.4633e-01,  2.9574e-01,  4.6955e-01, -2.0308e-01,  4.6772e-02,\n",
       "          8.1871e-01, -4.8880e-01,  4.1499e-01, -4.5076e-01, -1.4155e-01,\n",
       "          2.5151e-01, -2.5274e-01,  7.8204e-01, -6.3615e-01,  3.7210e-01,\n",
       "          3.5499e-01, -8.8430e-01,  1.2391e-01, -9.4578e-02, -2.4236e-01,\n",
       "         -6.7426e-01, -6.1039e-01, -1.7673e-01, -6.6865e-01, -2.7885e-01,\n",
       "          5.0805e-01, -3.3921e-01,  9.3489e-02,  3.7691e-01, -6.1615e-02,\n",
       "          1.1616e-01, -6.8762e-01, -2.5775e-01,  1.8795e-01, -6.9623e-02,\n",
       "         -2.3428e-01,  2.3471e-01,  2.7699e-01,  4.0835e-01, -6.0206e-01,\n",
       "         -3.1559e-01, -4.4148e-01,  9.2625e-02, -7.7604e-01,  7.5247e-01,\n",
       "         -3.4784e-01, -2.9181e-01, -1.3649e-01, -5.6352e-01,  7.6533e-03,\n",
       "          6.6498e-01, -1.8469e-01,  8.6314e-02,  5.9657e-01, -6.5804e-01,\n",
       "          2.6182e-01, -3.7584e-01, -4.7825e-01,  3.9715e-01, -1.7592e-01,\n",
       "          2.6041e-01,  1.7893e-01, -1.5762e-02,  3.3932e-02, -6.4228e-02,\n",
       "         -1.2057e-01,  3.4020e-01,  3.0019e-01, -3.5585e-01, -3.9305e-01,\n",
       "         -8.8588e-01, -8.0749e-02, -6.8382e-01,  5.7716e-01, -8.8196e-01,\n",
       "         -4.2831e-01, -1.9758e-01, -5.2855e-01,  2.9176e-01,  7.3715e-01,\n",
       "          1.4874e-01, -3.6739e-02, -1.3302e-01, -6.9399e-01,  2.9320e-01,\n",
       "         -4.5998e-01,  5.5820e-01, -1.8237e-01,  4.7171e-02,  2.7974e-01,\n",
       "         -2.6873e-01, -1.6112e-01,  7.9306e-01, -7.5923e-01, -6.6097e-01,\n",
       "         -6.0325e-02,  2.9259e-01,  8.3814e-01,  6.9768e-02,  2.9655e-01,\n",
       "          6.8221e-01, -7.2223e-01, -1.4918e-01, -9.0342e-01, -4.9295e-01,\n",
       "          3.9917e-01,  4.0683e-01, -2.9660e-01,  2.6952e-01, -3.6836e-01,\n",
       "         -3.4453e-01, -2.5244e-01,  9.2252e-01,  4.2580e-01, -2.3097e-01,\n",
       "         -2.3022e-01,  3.4273e-02, -5.5837e-01, -1.7409e-01,  2.0257e-02,\n",
       "         -9.5128e-02, -2.4292e-01,  6.1842e-01, -6.5253e-01,  5.2981e-01,\n",
       "          6.4060e-01, -6.9315e-01,  3.2877e-01, -7.6624e-01,  1.7028e-01,\n",
       "         -1.3934e-01,  3.4675e-01,  2.9761e-01,  5.0055e-01,  1.4617e-01,\n",
       "          6.6818e-02, -3.1182e-01, -3.5610e-01,  4.5260e-01,  1.0907e-01,\n",
       "          8.8470e-01,  7.1847e-01,  3.7007e-01, -3.9098e-01,  5.6396e-01,\n",
       "         -2.1773e-01, -6.0347e-01,  1.7910e-01,  1.1723e-01,  2.4811e-01,\n",
       "          4.6843e-01,  1.8712e-01,  1.6246e-01, -7.5787e-02,  2.6950e-01,\n",
       "          8.6982e-03, -5.3490e-01,  4.0633e-01, -7.1292e-01, -3.7946e-01,\n",
       "          5.2616e-01,  1.1131e-01, -6.7565e-01,  4.0259e-01, -1.2211e-01,\n",
       "         -1.1306e-02,  7.5006e-02, -3.3856e-01,  4.0554e-01, -9.3220e-02,\n",
       "         -6.3623e-01,  4.9200e-01,  1.1055e-01, -2.6392e-01,  1.6519e-01,\n",
       "          2.7617e-01, -3.0051e-01,  5.8814e-01,  3.5147e-03, -7.8505e-01,\n",
       "         -2.0492e-01,  6.0156e-01,  5.5387e-01,  3.2053e-02,  1.2759e-01,\n",
       "         -7.0448e-01,  4.0338e-01, -1.2790e-01,  5.2428e-01,  1.7253e-01,\n",
       "         -6.7519e-01, -5.3529e-01,  4.4249e-01, -7.0234e-02,  6.8174e-01,\n",
       "          4.6044e-02,  6.7626e-01, -5.1410e-01, -2.6933e-01, -1.4872e-01,\n",
       "          8.3963e-01,  3.0456e-01, -2.6591e-01,  3.4989e-01,  3.1685e-02,\n",
       "          5.2447e-03, -6.6699e-01, -3.2902e-01,  5.2520e-01, -1.1680e-01,\n",
       "          7.6002e-01, -4.0114e-01, -8.8250e-02, -3.8436e-01, -7.9385e-01,\n",
       "         -2.3931e-01,  1.7622e-01,  3.1946e-01,  5.7268e-01,  1.4803e-01,\n",
       "          2.0495e-01,  7.7191e-01, -4.7492e-01, -4.6504e-02, -3.7238e-01,\n",
       "         -3.0060e-01, -3.9502e-02, -3.8723e-01,  1.1616e-01, -3.9340e-01,\n",
       "         -1.6677e-01, -4.8682e-01, -4.3036e-01,  2.3243e-01, -1.6634e-01,\n",
       "         -8.5488e-02,  5.6731e-02, -4.1403e-01, -2.2941e-01, -4.4732e-03,\n",
       "          3.1065e-01, -2.5698e-01,  6.2466e-01,  5.3190e-01, -4.0741e-01,\n",
       "          5.5202e-01,  7.6314e-03, -1.4649e-01, -4.9081e-01,  8.3137e-01,\n",
       "         -4.9703e-01, -4.6178e-01, -2.8109e-01,  4.9070e-01,  6.4874e-01,\n",
       "         -8.1448e-01,  3.8059e-01, -2.4878e-01, -2.1170e-01, -1.7103e-01,\n",
       "          1.7326e-01, -9.6500e-02,  4.3680e-01, -3.1189e-01, -1.6027e-01,\n",
       "          6.8473e-01, -7.1751e-01, -5.7290e-01, -1.0337e-01, -7.7214e-01,\n",
       "          2.7153e-01, -4.7598e-01, -5.9594e-01,  2.7803e-01,  1.2125e-01,\n",
       "         -2.6909e-01, -3.5186e-01,  2.3822e-01,  3.6605e-01,  8.6452e-02,\n",
       "         -2.3978e-02, -3.3021e-01,  8.2654e-01, -5.0671e-01,  8.4709e-01,\n",
       "          3.4495e-01,  5.6795e-01, -4.8558e-01, -2.1427e-01,  5.4782e-01,\n",
       "          1.2531e-01, -5.8110e-01, -3.7883e-01,  7.7166e-01, -4.2583e-01,\n",
       "         -2.1068e-01,  3.3740e-01, -5.0828e-02, -5.8291e-01,  7.5717e-01,\n",
       "          7.6367e-01,  4.2239e-02,  5.0345e-01, -4.6831e-01,  1.4459e-01,\n",
       "         -6.1693e-01, -4.5239e-01,  5.7888e-01, -3.0008e-01, -2.1321e-01,\n",
       "         -2.8683e-01, -4.1005e-01,  1.5732e-01,  5.8152e-01, -8.6716e-01,\n",
       "          3.7028e-01,  5.9108e-02,  5.2018e-01,  4.0457e-01, -6.4091e-01,\n",
       "          2.2311e-01, -1.7763e-01,  8.8409e-02,  3.9174e-01,  2.9133e-02,\n",
       "         -3.8701e-01,  1.8685e-01, -5.8972e-01,  2.9444e-01, -5.0521e-01,\n",
       "          4.2299e-02, -2.3830e-01, -7.6447e-02,  2.1243e-01, -3.9048e-01,\n",
       "          1.5059e-02, -7.3542e-02,  6.2636e-01, -8.3289e-02, -2.8523e-01,\n",
       "          7.8642e-01, -3.7586e-01,  5.8801e-01,  5.5305e-01,  6.8738e-01,\n",
       "          8.4744e-02,  1.9354e-02,  5.9951e-01,  6.9784e-01,  2.4600e-01,\n",
       "         -7.5630e-01,  2.8803e-01,  6.2338e-01, -4.5702e-01, -4.7392e-01,\n",
       "          3.7606e-01,  2.1875e-01,  6.5555e-01,  8.1756e-01,  7.1803e-01,\n",
       "         -6.8440e-01, -5.3071e-02, -5.1707e-01,  3.6397e-01, -4.4673e-01,\n",
       "          1.9359e-01,  4.3042e-01,  6.4816e-01, -3.9844e-01,  3.8398e-01,\n",
       "          7.7086e-02,  2.3044e-01, -9.1848e-01, -6.2665e-01,  2.1387e-01,\n",
       "          7.8288e-02, -7.4854e-02,  7.4908e-01, -5.5568e-01, -4.7569e-01,\n",
       "         -5.5587e-01, -1.4950e-01,  5.7176e-02,  1.6273e-01, -6.3316e-01,\n",
       "         -7.1496e-02, -1.6120e-01,  1.3784e-01,  4.2198e-01, -2.0397e-01,\n",
       "         -2.9214e-01, -1.9454e-01,  1.6272e-01, -2.1364e-01,  5.5222e-01,\n",
       "         -8.7578e-01,  4.1631e-01,  8.4734e-01, -2.9092e-01,  2.8257e-01,\n",
       "         -7.7537e-01,  7.3905e-01,  6.0575e-01, -4.5452e-01, -8.1860e-01,\n",
       "         -4.0959e-01,  4.6900e-01, -5.1574e-01,  7.0919e-01,  1.1515e-01,\n",
       "          3.9174e-01,  3.1415e-01,  5.4840e-01, -1.0543e-01,  1.3218e-01,\n",
       "          7.5969e-01,  3.3672e-01, -3.6808e-01,  1.5985e-01,  1.5620e-01,\n",
       "          1.9492e-01, -7.1636e-01,  3.7513e-01,  8.7104e-02, -6.6841e-01,\n",
       "          2.6789e-01, -7.1122e-01, -4.3491e-01, -2.8127e-01, -4.8618e-01,\n",
       "         -7.4943e-02, -1.1696e-01, -2.8435e-01, -8.4809e-01, -5.1714e-01,\n",
       "         -9.1303e-01,  2.4577e-01,  6.4478e-01,  5.9679e-01,  2.2563e-01,\n",
       "         -2.5681e-01,  1.9443e-01,  3.9242e-01, -9.9937e-02, -5.4072e-01,\n",
       "          1.7852e-01, -9.0268e-02,  6.0166e-01,  4.9137e-01, -8.9963e-01,\n",
       "          7.8463e-01,  4.9963e-01,  1.5921e-01, -5.3403e-01, -1.8653e-01,\n",
       "          7.4914e-01, -7.7274e-01, -5.5906e-01,  5.4732e-01,  7.0107e-02,\n",
       "          4.0162e-01,  6.8716e-01,  2.6635e-01, -3.4943e-01,  2.4823e-01,\n",
       "         -5.2866e-01, -6.7906e-01,  2.3657e-01,  2.7691e-01,  2.3464e-01,\n",
       "          7.3293e-02,  7.4249e-01,  4.7609e-01, -1.1437e-01,  5.2901e-01,\n",
       "          1.1347e-01, -6.7678e-01, -4.2227e-01, -3.5853e-01, -7.8315e-01,\n",
       "         -4.3807e-02, -8.7219e-01, -5.3206e-01, -2.2873e-01, -1.0465e-01,\n",
       "          4.9312e-01,  6.0256e-01,  8.8773e-02, -7.3862e-01, -5.3669e-01,\n",
       "         -4.0186e-01, -6.4815e-01,  6.3016e-04, -2.4779e-01,  6.2436e-01,\n",
       "         -3.1829e-01, -8.5521e-01, -3.8313e-01,  4.0914e-02,  4.8201e-01,\n",
       "         -2.1527e-01, -9.0262e-01, -2.7855e-02,  1.0322e-01,  1.3151e-01,\n",
       "         -5.3416e-01,  5.0795e-01, -7.5806e-02, -1.0240e-01,  2.9667e-01,\n",
       "         -5.5380e-01, -5.7184e-01, -3.2265e-02,  3.4249e-01,  6.6084e-01,\n",
       "         -2.5674e-01, -2.4114e-01, -7.5168e-01, -3.1382e-01, -2.5875e-01,\n",
       "         -4.2257e-01, -9.0773e-02, -7.2824e-02,  1.1924e-01, -4.9435e-01,\n",
       "         -3.6359e-01, -4.8533e-01,  2.8616e-01,  7.3003e-01,  1.5917e-01,\n",
       "         -5.4976e-01,  4.4801e-01,  5.7851e-01,  2.8454e-01, -3.3731e-01,\n",
       "         -6.5820e-01, -1.5476e-01,  1.1745e-01,  7.2023e-02,  2.2826e-01,\n",
       "          3.9326e-01,  9.5315e-02,  5.9001e-01,  6.0742e-01,  1.7065e-01,\n",
       "          2.0512e-01,  2.5990e-01,  1.3177e-01,  3.2254e-01, -9.8649e-02,\n",
       "         -6.2918e-01,  5.0668e-01, -8.1668e-01, -1.1122e-01, -3.6646e-01,\n",
       "         -6.0003e-01,  8.3290e-01,  4.9222e-01, -4.4597e-01, -6.7376e-01,\n",
       "         -8.2182e-01,  4.1201e-01,  1.8402e-01,  3.9822e-01,  8.1154e-01,\n",
       "         -2.2929e-01, -7.2824e-01, -5.5268e-01, -1.8651e-02, -4.8606e-01,\n",
       "         -8.7519e-02,  3.7578e-01, -1.5730e-01, -4.8730e-01,  2.0852e-01,\n",
       "         -6.6081e-01,  6.8942e-01,  5.6557e-01]),\n",
       " tensor(0.6917))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Construct the path to the Documents folder\n",
    "documents_folder = os.path.join(home_dir, 'Documents')\n",
    "\n",
    "# Construct the full path for the ONNX files\n",
    "tensor_dataset_output_path = os.path.join(documents_folder, \"WELP-PLAPT/data\", \"tensor_dataset.data\")\n",
    "tensor_dataset_output_path_json = os.path.join(documents_folder, \"WELP-PLAPT/data\", \"tensor_dataset.json\")\n",
    "\n",
    "# Ensure the Encoders directory exists\n",
    "os.makedirs(os.path.join(documents_folder, \"WELP-PLAPT\", \"data\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tensor_dataset, tensor_dataset_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare two datasets\n",
    "def compare_datasets(dataset1, dataset2):\n",
    "    # Check if both are the same type\n",
    "    if type(dataset1) != type(dataset2):\n",
    "        return False\n",
    "\n",
    "    # If datasets are dictionaries\n",
    "    if isinstance(dataset1, dict):\n",
    "        if dataset1.keys() != dataset2.keys():\n",
    "            return False\n",
    "        for key in dataset1:\n",
    "            if not torch.equal(dataset1[key], dataset2[key]):\n",
    "                return False\n",
    "\n",
    "    # If datasets are lists or tuples\n",
    "    elif isinstance(dataset1, (list, tuple)):\n",
    "        if len(dataset1) != len(dataset2):\n",
    "            return False\n",
    "        for item1, item2 in zip(dataset1, dataset2):\n",
    "            if not torch.equal(item1, item2):\n",
    "                return False\n",
    "\n",
    "    # Add other comparisons if your dataset is of a different type\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: C:\\Users\\tatwo\\Documents\\WELP-PLAPT/data\\tensor_dataset.data\n",
      "File size is non-zero.\n",
      "Dataset successfully loaded.\n",
      "Success: The loaded dataset is identical to the original.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(tensor_dataset_output_path):\n",
    "    print(\"File does not exist:\", tensor_dataset_output_path)\n",
    "else:\n",
    "    print(\"File found:\", tensor_dataset_output_path)\n",
    "\n",
    "    # Check file size (should not be zero)\n",
    "    if os.path.getsize(tensor_dataset_output_path) == 0:\n",
    "        print(\"File is empty.\")\n",
    "    else:\n",
    "        print(\"File size is non-zero.\")\n",
    "\n",
    "        # Load the tensor dataset into a new variable\n",
    "        try:\n",
    "            loaded_tensor_dataset = torch.load(tensor_dataset_output_path)\n",
    "            print(\"Dataset successfully loaded.\")\n",
    "\n",
    "            # Compare the original and loaded datasets\n",
    "            if compare_datasets(tensor_dataset, loaded_tensor_dataset):\n",
    "                print(\"Success: The loaded dataset is identical to the original.\")\n",
    "            else:\n",
    "                print(\"Error: The loaded dataset differs from the original.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error loading the dataset:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Tensor Dataset to Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "\n",
    "tensor_dataset = torch.load(tensor_dataset_output_path)\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "prot_embeddings_list = []\n",
    "mol_embeddings_list = []\n",
    "affinity_list = []\n",
    "\n",
    "# Extract data from the TensorDataset\n",
    "for prot_emb, mol_emb, affinity in tensor_dataset:\n",
    "    # Flatten the feature tensors if they are multi-dimensional\n",
    "    prot_emb_flat = prot_emb.flatten().numpy()\n",
    "    mol_emb_flat = mol_emb.flatten().numpy()\n",
    "\n",
    "    # Append the flattened data and the label to the lists\n",
    "    prot_embeddings_list.append(prot_emb_flat)\n",
    "    mol_embeddings_list.append(mol_emb_flat)\n",
    "    affinity_list.append(affinity.item())  # Assuming affinity is a single value\n",
    "\n",
    "# Create a DataFrame\n",
    "# You might need to adjust this part depending on the structure of your features\n",
    "df = pd.DataFrame({\n",
    "    'prot_embeddings': prot_embeddings_list,\n",
    "    'mol_embeddings': mol_embeddings_list,\n",
    "    'affinity': affinity_list\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(tensor_dataset_output_path_json, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
