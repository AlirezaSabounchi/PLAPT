{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 267,  21, 298,  22,  71,  12,  82,  21,  13, 286,  12,  71,  21,\n",
       "         270,  12,  39, 263,  51,  13,  50,  63,  39,  36,  44, 265,  39,  13,\n",
       "          71,  23, 264,  12,  39,  12,  42, 281,  42,  13,  42,  13, 261,  23,\n",
       "          13, 261, 263,  51, 272, 290,  65,  21,  13,  39,  22,   2],\n",
       "        [  0, 262,  12,  39,  13, 286,  12, 267,  21, 261,  12, 278,  13,  71,\n",
       "          22,  71,  12,  71,  21,  13, 420,  22,  13,  39, 263,  51,  13,  39,\n",
       "          12,  39,  13, 463,  21, 276,  22,  63, 290,  65, 298, 304,   2,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer([\"Cc1ncc2c(n1)CN(c1nc(C(=O)N[C@H](C)c3ccc(C(F)(F)F)cc3)cc(=O)[nH]1)C2\",\"CC(C)CN(Cc1cc(Cl)c2c(c1)OCCCO2)C(=O)C(C)CNCc1cccc2[nH]ncc12\"], padding=True, return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass tokens through model\n",
    "with torch.no_grad():  # Deactivate gradients for the following code\n",
    "    outputs = model(**tokens).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2971, -0.8034, -0.4948,  ..., -0.5058,  0.6291,  0.3167],\n",
       "        [ 0.2814, -0.9092, -0.0643,  ..., -0.3179, -0.1356, -0.2319]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last hidden state is the feature vector we're interested in\n",
    "len(outputs[0])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
