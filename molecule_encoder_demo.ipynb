{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Manim\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 267,  21, 298,  22,  71,  12,  82,  21,  13, 286,  12,  71,  21,\n",
       "         270,  12,  39, 263,  51,  13,  50,  63,  39,  36,  44, 265,  39,  13,\n",
       "          71,  23, 264,  12,  39,  12,  42, 281,  42,  13,  42,  13, 261,  23,\n",
       "          13, 261, 263,  51, 272, 290,  65,  21,  13,  39,  22,   2],\n",
       "        [  0, 262,  12,  39,  13, 286,  12, 267,  21, 261,  12, 278,  13,  71,\n",
       "          22,  71,  12,  71,  21,  13, 420,  22,  13,  39, 263,  51,  13,  39,\n",
       "          12,  39,  13, 463,  21, 276,  22,  63, 290,  65, 298, 304,   2,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer([\"Cc1ncc2c(n1)CN(c1nc(C(=O)N[C@H](C)c3ccc(C(F)(F)F)cc3)cc(=O)[nH]1)C2\",\"CC(C)CN(Cc1cc(Cl)c2c(c1)OCCCO2)C(=O)C(C)CNCc1cccc2[nH]ncc12\"], padding=True, return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2583, -0.7915, -0.0595,  ..., -1.3572,  0.4673,  0.8433],\n",
       "         [ 1.2943, -0.5931, -0.7251,  ...,  0.9675,  0.5271, -0.6003],\n",
       "         [ 0.0241,  0.5063,  0.2055,  ...,  1.1754, -0.0095, -1.0407],\n",
       "         ...,\n",
       "         [ 0.8738, -0.1259, -0.2457,  ...,  0.8148, -1.4340,  2.1456],\n",
       "         [ 0.7482, -0.7841, -1.3179,  ...,  0.4312,  0.6104,  1.5554],\n",
       "         [ 2.8033,  0.1085, -0.7306,  ..., -0.4948, -0.6792,  1.0854]],\n",
       "\n",
       "        [[ 1.8713,  0.7965, -0.4507,  ..., -0.4820, -0.7786,  1.2478],\n",
       "         [ 1.5011, -0.7081, -0.4598,  ...,  0.7532,  0.4367, -0.7854],\n",
       "         [ 3.1207, -0.1093, -0.2326,  ...,  0.5469, -0.1305, -0.0347],\n",
       "         ...,\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139],\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139],\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass tokens through model\n",
    "with torch.no_grad():  # Deactivate gradients for the following code\n",
    "    outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.2583, -0.7915, -0.0595,  ..., -1.3572,  0.4673,  0.8433],\n",
       "         [ 1.2943, -0.5931, -0.7251,  ...,  0.9675,  0.5271, -0.6003],\n",
       "         [ 0.0241,  0.5063,  0.2055,  ...,  1.1754, -0.0095, -1.0407],\n",
       "         ...,\n",
       "         [ 0.8738, -0.1259, -0.2457,  ...,  0.8148, -1.4340,  2.1456],\n",
       "         [ 0.7482, -0.7841, -1.3179,  ...,  0.4312,  0.6104,  1.5554],\n",
       "         [ 2.8033,  0.1085, -0.7306,  ..., -0.4948, -0.6792,  1.0854]],\n",
       "\n",
       "        [[ 1.8713,  0.7965, -0.4507,  ..., -0.4820, -0.7786,  1.2478],\n",
       "         [ 1.5011, -0.7081, -0.4598,  ...,  0.7532,  0.4367, -0.7854],\n",
       "         [ 3.1207, -0.1093, -0.2326,  ...,  0.5469, -0.1305, -0.0347],\n",
       "         ...,\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139],\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139],\n",
       "         [ 1.0350,  0.9091, -0.9591,  ..., -0.8376, -0.6618,  1.4139]]]), pooler_output=tensor([[ 0.2971, -0.8034, -0.4948,  ..., -0.5058,  0.6291,  0.3167],\n",
       "        [ 0.2814, -0.9092, -0.0643,  ..., -0.3179, -0.1356, -0.2319]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last hidden state is the feature vector we're interested in\n",
    "len(outputs[0])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,             \n",
    "                      args=(tokens['input_ids'],),\n",
    "                      f=\"molecule_embedding.onnx\",   \n",
    "                      input_names=['input_ids'],   \n",
    "                      output_names=['molecule_embeddings'], \n",
    "                      opset_version=11)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "token_ids = tokens['input_ids'].numpy()  # Convert to numpy array\n",
    "df = pd.DataFrame(token_ids)\n",
    "\n",
    "\n",
    "file_path = r'Data\\molecule\\example.csv'\n",
    "\n",
    "df.to_csv(file_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
