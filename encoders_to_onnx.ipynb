{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export ProtBERT and ChemBERTa encoder models to ONNX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "mol_encoder = RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_encoder = BertModel.from_pretrained(\"Rostlab/prot_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_prot_input_size = prot_model.config.max_position_embeddings\n",
    "max_prot_input_size = 3200 #capped at 3200 since tokens longer than 3200 use way too much vram and cause bottleneck\n",
    "max_mol_input_size = 278 # this is the length of the tokenized longest molecule in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy inputs\n",
    "dummy_input_mol = mol_tokenizer(\"CCCCCCCCCCCCCCCCCCCC(=O)O\", padding='max_length', max_length=max_mol_input_size, return_tensors=\"pt\")\n",
    "dummy_input_prot = prot_tokenizer([\"M T V P D R S E I A G K W Y V V A L A S N T E F F L R E K D K M K M A M A R I S F L G E D E L K V S Y A V P K P N G C R K W E T T F K K T S D D G E V Y Y S E E A K K K V E V L D T D Y K S Y A V I Y A T R V K D G R T L H M M R L Y S R S P E V S P A A T A I F R K L A G E R N Y T D E M V A M L P R Q E E C T V D E V\"], padding='max_length', max_length=max_prot_input_size, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "mol_encoder.eval();\n",
    "prot_encoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models will save to your documents folder, with the path **\"../Documents/WELP-PLAPT/models\"**\n",
    "\n",
    "Might be broken on mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Construct the path to the Documents folder\n",
    "documents_folder = os.path.join(home_dir, 'Documents')\n",
    "\n",
    "# Construct the full path for the ONNX files\n",
    "prot_encoder_output_path = os.path.join(documents_folder, \"WELP-PLAPT/models\", \"prot_encoder.onnx\")\n",
    "mol_encoder_output_path = os.path.join(documents_folder, \"WELP-PLAPT/models\", \"mol_encoder.onnx\")\n",
    "\n",
    "# Ensure the Encoders directory exists\n",
    "os.makedirs(os.path.join(documents_folder, \"WELP-PLAPT\", \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Molecular Encoder to ONNX\n",
    "torch.onnx.export(mol_encoder, \n",
    "                  args=(dummy_input_mol['input_ids'], dummy_input_mol['attention_mask']), \n",
    "                  f=mol_encoder_output_path,\n",
    "                  input_names=['input_ids', 'attention_mask'],\n",
    "                  output_names=['output'],\n",
    "                  opset_version=15,  # or another version depending on compatibility\n",
    "                  do_constant_folding=True,  # optimize the model\n",
    "                  # dynamic axes may not be supported in wolfram language mathematica.\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, \n",
    "                                'attention_mask': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Protein Encoder to ONNX\n",
    "torch.onnx.export(prot_encoder, \n",
    "                  args=(dummy_input_prot['input_ids'], dummy_input_prot['attention_mask'], dummy_input_prot['token_type_ids']), \n",
    "                  f=prot_encoder_output_path,\n",
    "                  input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "                  output_names=['output'],\n",
    "                  opset_version=15, \n",
    "                  do_constant_folding=True,\n",
    "                  # dynamic axes may not be supported in wolfram language mathematica.\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, \n",
    "                                'attention_mask': {0: 'batch_size'},\n",
    "                                'token_type_ids': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    original_prot_output = prot_encoder(**dummy_input_prot)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    original_mol_output = mol_encoder(**dummy_input_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prot_output = original_prot_output[0]\n",
    "original_mol_output = original_mol_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Function to run ONNX inference\n",
    "def run_onnx_inference(onnx_model_path, dummy_input):\n",
    "    session = ort.InferenceSession(onnx_model_path)\n",
    "    inputs = {session.get_inputs()[0].name: dummy_input['input_ids'].cpu().numpy(),\n",
    "              session.get_inputs()[1].name: dummy_input['attention_mask'].cpu().numpy()}\n",
    "    if len(session.get_inputs()) == 3:\n",
    "        inputs[session.get_inputs()[2].name] = dummy_input['token_type_ids'].cpu().numpy()\n",
    "    onnx_output = session.run(None, inputs)\n",
    "    return onnx_output[0]\n",
    "\n",
    "# Run inference for protein encoder\n",
    "onnx_prot_output = run_onnx_inference(prot_encoder_output_path, dummy_input_prot)\n",
    "# Run inference for molecule encoder\n",
    "onnx_mol_output = run_onnx_inference(mol_encoder_output_path, dummy_input_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Encoder Outputs Similar: True\n",
      "Molecule Encoder Outputs Similar: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compare outputs\n",
    "def compare_outputs(original_output, onnx_output, threshold=1e-3):\n",
    "    return np.allclose(original_output, onnx_output, atol=threshold)\n",
    "\n",
    "# Compare the outputs\n",
    "is_prot_output_similar = compare_outputs(original_prot_output.cpu().numpy(), onnx_prot_output)\n",
    "print(\"Protein Encoder Outputs Similar:\", is_prot_output_similar)\n",
    "\n",
    "is_mol_output_similar = compare_outputs(original_mol_output.cpu().numpy(), onnx_mol_output)\n",
    "print(\"Molecule Encoder Outputs Similar:\", is_mol_output_similar)\n",
    "\n",
    "if (is_prot_output_similar and is_mol_output_similar):\n",
    "    print(\"Passed!\")\n",
    "else:\n",
    "    print(\"Failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
