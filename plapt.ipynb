{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, AdamW\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# prot_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "# mol_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "# mol_model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "# dataset = load_dataset(\"jglaser/binding_affinity\")['train'].train_test_split(test_size=0.001)['test']\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"jglaser/binding_affinity\")\n",
    "\n",
    "# Load the tokenizers\n",
    "protein_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "smiles_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2294de1d89454d2392f8ed14e6a738bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1836729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'protein_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\tatwo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tatwo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py\", line 1354, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\tatwo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py\", line 3474, in _map_single\n    batch = apply_function_on_filtered_inputs(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tatwo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\tatwo\\AppData\\Local\\Temp\\ipykernel_18112\\2436433575.py\", line 3, in tokenize_function\nNameError: name 'protein_tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\Other computers\\Home PC\\Programs\\Wolfram\\WELP-PLAPT\\plapt.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mprotein_input\u001b[39m\u001b[39m'\u001b[39m: protein_input, \u001b[39m'\u001b[39m\u001b[39msmiles_input\u001b[39m\u001b[39m'\u001b[39m: smiles_input}\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Tokenize the sequences and SMILES strings using multiple cores\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_proc\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Split the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# train_dataset = dataset['train'].train_test_split(test_size=0.1)['train']\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# val_dataset = dataset['train'].train_test_split(test_size=0.1)['test']\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Other%20computers/Home%20PC/Programs/Wolfram/WELP-PLAPT/plapt.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtrain_test_split(test_size\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     {\n\u001b[0;32m    854\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;49;00m k, dataset \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3189\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3182\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3183\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3184\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3185\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3186\u001b[0m     total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3187\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3188\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3189\u001b[0m     \u001b[39mfor\u001b[39;49;00m rank, done, content \u001b[39min\u001b[39;49;00m iflatmap_unordered(\n\u001b[0;32m   3190\u001b[0m         pool, Dataset\u001b[39m.\u001b[39;49m_map_single, kwargs_iterable\u001b[39m=\u001b[39;49mkwargs_per_job\n\u001b[0;32m   3191\u001b[0m     ):\n\u001b[0;32m   3192\u001b[0m         \u001b[39mif\u001b[39;49;00m done:\n\u001b[0;32m   3193\u001b[0m             shards_done \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py:1394\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1393\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m         [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m async_result \u001b[39min\u001b[39;49;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py:1394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1393\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m         [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'protein_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    protein_input = protein_tokenizer(examples['seq'], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    smiles_input = smiles_tokenizer(examples['smiles_can'], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    return {'protein_input': protein_input, 'smiles_input': smiles_input}\n",
    "\n",
    "# Tokenize the sequences and SMILES strings using multiple cores\n",
    "dataset = dataset.map(tokenize_function, batched=True, num_proc=20)\n",
    "\n",
    "# Split the dataset\n",
    "# train_dataset = dataset['train'].train_test_split(test_size=0.1)['train']\n",
    "# val_dataset = dataset['train'].train_test_split(test_size=0.1)['test']\n",
    "\n",
    "train_dataset = dataset['train'].train_test_split(test_size=0.0001)['test']\n",
    "val_dataset = dataset['train'].train_test_split(test_size=0.00001)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BindingAffinityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BindingAffinityModel, self).__init__()\n",
    "\n",
    "        self.protein_encoder = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "        self.smiles_encoder = BertModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "        # Considering both the models' embedding sizes are 768 (standard BERT-base size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768 * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)  # Final binding affinity value\n",
    "        )\n",
    "\n",
    "    def forward(self, protein_input, smiles_input):\n",
    "        protein_embedding = self.protein_encoder(**protein_input).last_hidden_state[:,0,:]\n",
    "        smiles_embedding = self.smiles_encoder(**smiles_input).last_hidden_state[:,0,:]\n",
    "        \n",
    "        concatenated_embeddings = torch.cat((protein_embedding, smiles_embedding), dim=1)\n",
    "        \n",
    "        return self.fc(concatenated_embeddings)\n",
    "\n",
    "model = BindingAffinityModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['seq',\n",
       "  'smiles',\n",
       "  'affinity_uM',\n",
       "  'neg_log10_affinity_M',\n",
       "  'smiles_can',\n",
       "  'affinity'],\n",
       " 'no_kras': ['seq',\n",
       "  'smiles',\n",
       "  'affinity_uM',\n",
       "  'neg_log10_affinity_M',\n",
       "  'smiles_can',\n",
       "  'affinity'],\n",
       " 'covalent': ['seq',\n",
       "  'smiles',\n",
       "  'affinity_uM',\n",
       "  'neg_log10_affinity_M',\n",
       "  'smiles_can',\n",
       "  'affinity']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch['protein_input'], batch['smiles_input'])\n",
    "        loss = criterion(outputs, batch['affinity'])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation logic here (compute metrics on validation set)\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am doing a machine learning project in python where for predicting protein ligand binding affinity. Here is the code I have right now:\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "mol_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "mol_model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "dataset = load_dataset(\"jglaser/binding_affinity\")\n",
    "\n",
    "the dataset has the following column names and structure:\n",
    "{'train': ['seq',\n",
    "  'smiles',\n",
    "  'affinity_uM',\n",
    "  'neg_log10_affinity_M',\n",
    "  'smiles_can',\n",
    "  'affinity'],\n",
    " 'no_kras': ['seq',\n",
    "  'smiles',\n",
    "  'affinity_uM',\n",
    "  'neg_log10_affinity_M',\n",
    "  'smiles_can',\n",
    "  'affinity'],\n",
    " 'covalent': ['seq',\n",
    "  'smiles',\n",
    "  'affinity_uM',\n",
    "  'neg_log10_affinity_M',\n",
    "  'smiles_can',\n",
    "  'affinity']}\n",
    "\n",
    "I want to use the prot_model as an encoder for the sequences and the mol_model for an encoder for the smiles_can data. The model will then predict affinity values using a cross attention mechanism from the encoded sequence and molecule data. I want you to write the entire program. \n",
    "\n",
    "You are an experienced software engineer and machine learning engineer. \n",
    "\n",
    "Take a deep breath and work on this step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
